{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Training and Prediction with Sagemaker Scikit-learn\n",
    "### Modified Version of AWS Example:\n",
    "https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_iris/Scikit-learn%20Estimator%20Example%20With%20Batch%20Transform.ipynb  \n",
    "\n",
    "Following modifications were made:  \n",
    "1. Incorporated scripts for local mode hosting  \n",
    "2. Added Train and Test Channels  \n",
    "3. Visualize results (confusion matrix and reports)  \n",
    "4. Added steps to deploy using model artifacts stored in S3  \n",
    "  \n",
    "Following Script changes were made:  \n",
    "1. RandomForest Algorithm\n",
    "2. Refactored script to follow the template provided in tensorflow example:\n",
    "https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_script_mode_training_and_serving/tensorflow_script_mode_training_and_serving.ipynb\n",
    "\n",
    "\n",
    "This tutorial shows you how to use [Scikit-learn](https://scikit-learn.org/stable/) with Sagemaker by utilizing the pre-built container. Scikit-learn is a popular Python machine learning framework. It includes a number of different algorithms for classification, regression, clustering, dimensionality reduction, and data/feature pre-processing. \n",
    "\n",
    "The [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) module  makes it easy to take existing scikit-learn code, which we will show by training a model on the IRIS dataset and generating a set of predictions. For more information about the Scikit-learn container, see the [sagemaker-scikit-learn-containers](https://github.com/aws/sagemaker-scikit-learn-container) repository and the [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) repository.\n",
    "\n",
    "For more on Scikit-learn, please visit the Scikit-learn website: <http://scikit-learn.org/stable/>.\n",
    "\n",
    "### Table of contents\n",
    "* [Upload the data for training](#upload_data)\n",
    "* [Create a Scikit-learn script to train with](#create_sklearn_script)\n",
    "* [Create the SageMaker Scikit Estimator](#create_sklearn_estimator)\n",
    "* [Train the SKLearn Estimator on the Iris data](#train_sklearn)\n",
    "* [Using the trained model to make inference requests](#inferece)\n",
    " * [Deploy the model](#deploy)\n",
    " * [Choose some data and use it for a prediction](#prediction_request)\n",
    " * [Endpoint cleanup](#endpoint_cleanup)\n",
    "* [Batch Transform](#batch_transform)\n",
    " * [Prepare Input Data](#prepare_input_data)\n",
    " * [Run Transform Job](#run_transform_job)\n",
    " * [Check Output Data](#check_output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets create our Sagemaker session and role, and create a S3 prefix to use for the notebook example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Mode Execution - requires docker compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook Setup. No longer needed as Jupyter Notebook\n",
    "# has docker pre-installed for local mode execution\n",
    "\n",
    "#!/bin/bash ./setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]      \n",
      "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3413 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2738 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [52.2 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1513 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.8 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3323 kB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2454 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1224 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [45.2 kB]\n",
      "Fetched 15.2 MB in 1s (10.5 MB/s)                       \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  dirmngr gnupg-l10n gnupg-utils gpg gpg-agent gpg-wks-client gpg-wks-server\n",
      "  gpgconf gpgsm libassuan0 libcurl4 libksba8 libnpth0 libreadline8\n",
      "  libsqlite3-0 openssl pinentry-curses readline-common\n",
      "Suggested packages:\n",
      "  dbus-user-session libpam-systemd pinentry-gnome3 tor parcimonie xloadimage\n",
      "  scdaemon pinentry-doc readline-doc\n",
      "The following NEW packages will be installed:\n",
      "  ca-certificates dirmngr gnupg gnupg-l10n gnupg-utils gpg gpg-agent\n",
      "  gpg-wks-client gpg-wks-server gpgconf gpgsm libassuan0 libksba8 libnpth0\n",
      "  libreadline8 libsqlite3-0 openssl pinentry-curses readline-common\n",
      "The following packages will be upgraded:\n",
      "  curl libcurl4\n",
      "2 upgraded, 19 newly installed, 0 to remove and 2 not upgraded.\n",
      "Need to get 4987 kB of archives.\n",
      "After this operation, 10.5 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openssl amd64 3.0.2-0ubuntu1.18 [1184 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ca-certificates all 20240203~22.04.1 [162 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 readline-common all 8.1.2-1 [53.5 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libreadline8 amd64 8.1.2-1 [153 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsqlite3-0 amd64 3.37.2-2ubuntu0.3 [641 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 curl amd64 7.81.0-1ubuntu1.19 [194 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libcurl4 amd64 7.81.0-1ubuntu1.19 [289 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libassuan0 amd64 2.5.5-1build1 [38.2 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpgconf amd64 2.2.27-3ubuntu2.1 [94.2 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libksba8 amd64 1.6.0-2ubuntu0.2 [119 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnpth0 amd64 1.6-3build2 [8664 B]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 dirmngr amd64 2.2.27-3ubuntu2.1 [293 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gnupg-l10n all 2.2.27-3ubuntu2.1 [54.4 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gnupg-utils amd64 2.2.27-3ubuntu2.1 [308 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpg amd64 2.2.27-3ubuntu2.1 [519 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 pinentry-curses amd64 1.1.1-1build2 [34.4 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpg-agent amd64 2.2.27-3ubuntu2.1 [209 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpg-wks-client amd64 2.2.27-3ubuntu2.1 [62.7 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpg-wks-server amd64 2.2.27-3ubuntu2.1 [57.5 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpgsm amd64 2.2.27-3ubuntu2.1 [197 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gnupg all 2.2.27-3ubuntu2.1 [315 kB]\n",
      "Fetched 4987 kB in 0s (20.9 MB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package openssl.\n",
      "(Reading database ... 13790 files and directories currently installed.)\n",
      "Preparing to unpack .../00-openssl_3.0.2-0ubuntu1.18_amd64.deb ...\n",
      "Unpacking openssl (3.0.2-0ubuntu1.18) ...\n",
      "Selecting previously unselected package ca-certificates.\n",
      "Preparing to unpack .../01-ca-certificates_20240203~22.04.1_all.deb ...\n",
      "Unpacking ca-certificates (20240203~22.04.1) ...\n",
      "Selecting previously unselected package readline-common.\n",
      "Preparing to unpack .../02-readline-common_8.1.2-1_all.deb ...\n",
      "Unpacking readline-common (8.1.2-1) ...\n",
      "Selecting previously unselected package libreadline8:amd64.\n",
      "Preparing to unpack .../03-libreadline8_8.1.2-1_amd64.deb ...\n",
      "Unpacking libreadline8:amd64 (8.1.2-1) ...\n",
      "Selecting previously unselected package libsqlite3-0:amd64.\n",
      "Preparing to unpack .../04-libsqlite3-0_3.37.2-2ubuntu0.3_amd64.deb ...\n",
      "Unpacking libsqlite3-0:amd64 (3.37.2-2ubuntu0.3) ...\n",
      "Preparing to unpack .../05-curl_7.81.0-1ubuntu1.19_amd64.deb ...\n",
      "Unpacking curl (7.81.0-1ubuntu1.19) over (7.81.0-1ubuntu1.18) ...\n",
      "Preparing to unpack .../06-libcurl4_7.81.0-1ubuntu1.19_amd64.deb ...\n",
      "Unpacking libcurl4:amd64 (7.81.0-1ubuntu1.19) over (7.81.0-1ubuntu1.18) ...\n",
      "Selecting previously unselected package libassuan0:amd64.\n",
      "Preparing to unpack .../07-libassuan0_2.5.5-1build1_amd64.deb ...\n",
      "Unpacking libassuan0:amd64 (2.5.5-1build1) ...\n",
      "Selecting previously unselected package gpgconf.\n",
      "Preparing to unpack .../08-gpgconf_2.2.27-3ubuntu2.1_amd64.deb ...\n",
      "Unpacking gpgconf (2.2.27-3ubuntu2.1) ...\n",
      "Selecting previously unselected package libksba8:amd64.\n",
      "Preparing to unpack .../09-libksba8_1.6.0-2ubuntu0.2_amd64.deb ...\n",
      "Unpacking libksba8:amd64 (1.6.0-2ubuntu0.2) ...\n",
      "Selecting previously unselected package libnpth0:amd64.\n",
      "Preparing to unpack .../10-libnpth0_1.6-3build2_amd64.deb ...\n",
      "Unpacking libnpth0:amd64 (1.6-3build2) ...\n",
      "Selecting previously unselected package dirmngr.\n",
      "Preparing to unpack .../11-dirmngr_2.2.27-3ubuntu2.1_amd64.deb ...\n",
      "Unpacking dirmngr (2.2.27-3ubuntu2.1) ...\n",
      "Selecting previously unselected package gnupg-l10n.\n",
      "Preparing to unpack .../12-gnupg-l10n_2.2.27-3ubuntu2.1_all.deb ...\n",
      "Unpacking gnupg-l10n (2.2.27-3ubuntu2.1) ...\n",
      "Selecting previously unselected package gnupg-utils.\n",
      "Preparing to unpack .../13-gnupg-utils_2.2.27-3ubuntu2.1_amd64.deb ...\n",
      "Unpacking gnupg-utils (2.2.27-3ubuntu2.1) ...\n",
      "Selecting previously unselected package gpg.\n",
      "Preparing to unpack .../14-gpg_2.2.27-3ubuntu2.1_amd64.deb ...\n",
      "Unpacking gpg (2.2.27-3ubuntu2.1) ...\n",
      "Selecting previously unselected package pinentry-curses.\n",
      "Preparing to unpack .../15-pinentry-curses_1.1.1-1build2_amd64.deb ...\n",
      "Unpacking pinentry-curses (1.1.1-1build2) ...\n",
      "Selecting previously unselected package gpg-agent.\n",
      "Preparing to unpack .../16-gpg-agent_2.2.27-3ubuntu2.1_amd64.deb ...\n",
      "Unpacking gpg-agent (2.2.27-3ubuntu2.1) ...\n",
      "Selecting previously unselected package gpg-wks-client.\n",
      "Preparing to unpack .../17-gpg-wks-client_2.2.27-3ubuntu2.1_amd64.deb ...\n",
      "Unpacking gpg-wks-client (2.2.27-3ubuntu2.1) ...\n",
      "Selecting previously unselected package gpg-wks-server.\n",
      "Preparing to unpack .../18-gpg-wks-server_2.2.27-3ubuntu2.1_amd64.deb ...\n",
      "Unpacking gpg-wks-server (2.2.27-3ubuntu2.1) ...\n",
      "Selecting previously unselected package gpgsm.\n",
      "Preparing to unpack .../19-gpgsm_2.2.27-3ubuntu2.1_amd64.deb ...\n",
      "Unpacking gpgsm (2.2.27-3ubuntu2.1) ...\n",
      "Selecting previously unselected package gnupg.\n",
      "Preparing to unpack .../20-gnupg_2.2.27-3ubuntu2.1_all.deb ...\n",
      "Unpacking gnupg (2.2.27-3ubuntu2.1) ...\n",
      "Setting up libksba8:amd64 (1.6.0-2ubuntu0.2) ...\n",
      "Setting up libsqlite3-0:amd64 (3.37.2-2ubuntu0.3) ...\n",
      "Setting up libnpth0:amd64 (1.6-3build2) ...\n",
      "Setting up libassuan0:amd64 (2.5.5-1build1) ...\n",
      "Setting up gnupg-l10n (2.2.27-3ubuntu2.1) ...\n",
      "Setting up libcurl4:amd64 (7.81.0-1ubuntu1.19) ...\n",
      "Setting up curl (7.81.0-1ubuntu1.19) ...\n",
      "Setting up openssl (3.0.2-0ubuntu1.18) ...\n",
      "Setting up readline-common (8.1.2-1) ...\n",
      "Setting up pinentry-curses (1.1.1-1build2) ...\n",
      "Setting up libreadline8:amd64 (8.1.2-1) ...\n",
      "Setting up ca-certificates (20240203~22.04.1) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Updating certificates in /etc/ssl/certs...\n",
      "rehash: warning: skipping ca-certificates.crt,it does not contain exactly one certificate or CRL\n",
      "146 added, 0 removed; done.\n",
      "Setting up gpgconf (2.2.27-3ubuntu2.1) ...\n",
      "Setting up gpg (2.2.27-3ubuntu2.1) ...\n",
      "Setting up gnupg-utils (2.2.27-3ubuntu2.1) ...\n",
      "Setting up gpg-agent (2.2.27-3ubuntu2.1) ...\n",
      "Setting up gpgsm (2.2.27-3ubuntu2.1) ...\n",
      "Setting up dirmngr (2.2.27-3ubuntu2.1) ...\n",
      "Setting up gpg-wks-server (2.2.27-3ubuntu2.1) ...\n",
      "Setting up gpg-wks-client (2.2.27-3ubuntu2.1) ...\n",
      "Setting up gnupg (2.2.27-3ubuntu2.1) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
      "Processing triggers for ca-certificates (20240203~22.04.1) ...\n",
      "Updating certificates in /etc/ssl/certs...\n",
      "0 added, 0 removed; done.\n",
      "Running hooks in /etc/ca-certificates/update.d...\n",
      "done.\n",
      "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:2 https://download.docker.com/linux/ubuntu jammy InRelease [48.8 kB]       \n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease                 \n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease             \n",
      "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease             \n",
      "Get:6 https://download.docker.com/linux/ubuntu jammy/stable amd64 Packages [48.8 kB]\n",
      "Fetched 97.7 kB in 1s (153 kB/s)\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  docker-ce-cli docker-compose-plugin\n",
      "0 upgraded, 2 newly installed, 0 to remove and 2 not upgraded.\n",
      "Need to get 55.8 MB of archives.\n",
      "After this operation, 218 MB of additional disk space will be used.\n",
      "Get:1 https://download.docker.com/linux/ubuntu jammy/stable amd64 docker-ce-cli amd64 5:20.10.24~3-0~ubuntu-jammy [43.2 MB]\n",
      "Get:2 https://download.docker.com/linux/ubuntu jammy/stable amd64 docker-compose-plugin amd64 2.29.7-1~ubuntu.22.04~jammy [12.7 MB]\n",
      "Fetched 55.8 MB in 1s (65.4 MB/s)               \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package docker-ce-cli.\n",
      "(Reading database ... 14505 files and directories currently installed.)\n",
      "Preparing to unpack .../docker-ce-cli_5%3a20.10.24~3-0~ubuntu-jammy_amd64.deb ...\n",
      "Unpacking docker-ce-cli (5:20.10.24~3-0~ubuntu-jammy) ...\n",
      "Selecting previously unselected package docker-compose-plugin.\n",
      "Preparing to unpack .../docker-compose-plugin_2.29.7-1~ubuntu.22.04~jammy_amd64.deb ...\n",
      "Unpacking docker-compose-plugin (2.29.7-1~ubuntu.22.04~jammy) ...\n",
      "Setting up docker-compose-plugin (2.29.7-1~ubuntu.22.04~jammy) ...\n",
      "Setting up docker-ce-cli (5:20.10.24~3-0~ubuntu-jammy) ...\n",
      "Client: Docker Engine - Community\n",
      " Version:           20.10.24\n",
      " API version:       1.41\n",
      " Go version:        go1.19.7\n",
      " Git commit:        297e128\n",
      " Built:             Tue Apr  4 18:21:03 2023\n",
      " OS/Arch:           linux/amd64\n",
      " Context:           default\n",
      " Experimental:      true\n",
      "Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\n"
     ]
    }
   ],
   "source": [
    "# JupyterLab: Use this script to install docker components in JupyterLab\n",
    "# This is needed for local mode training and deployment\n",
    "# Modified to incorporate additional checks: https://github.com/aws-samples/amazon-sagemaker-local-mode/blob/main/sagemaker_studio_docker_cli_install/sagemaker-ubuntu-jammy-docker-cli-install.sh\n",
    "!/bin/bash \"../../docker_install.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# SageMaker SKLearn Estimator\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Python version', sys.version)\n",
    "print ('Sklearn version', sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list_file = 'iris_train_column_list.txt'\n",
    "train_file = 'iris_train.csv'\n",
    "test_file = 'iris_validation.csv'\n",
    "\n",
    "columns = ''\n",
    "with open(column_list_file,'r') as f:\n",
    "    columns = f.read().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your bucket name\n",
    "bucket_name = 'chandra-ml-sagemaker-us-west-2'\n",
    "\n",
    "training_folder = r'iris/train'\n",
    "test_folder = r'iris/test'\n",
    "model_folder = r'iris/model/'\n",
    "\n",
    "training_data_uri = r's3://' + bucket_name + r'/' + training_folder\n",
    "testing_data_uri = r's3://' + bucket_name + r'/' + test_folder\n",
    "model_data_uri = r's3://' + bucket_name + r'/' + model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_uri,testing_data_uri,model_data_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.upload_data(train_file,\n",
    "                              bucket=bucket_name, \n",
    "                              key_prefix=training_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.upload_data(test_file, \n",
    "                              bucket=bucket_name, \n",
    "                              key_prefix=test_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the data locally, we can use use the tools provided by the SageMaker Python SDK to upload the data to a default bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Scikit-learn script to train with <a class=\"anchor\" id=\"create_sklearn_script\"></a>\n",
    "SageMaker can now run a scikit-learn script using the `SKLearn` estimator. When executed on SageMaker a number of helpful environment variables are available to access properties of the training environment, such as:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to. Any artifacts saved in this folder are uploaded to S3 for model hosting after the training job completes.\n",
    "* `SM_OUTPUT_DIR`: A string representing the filesystem path to write output artifacts to. Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts. These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.\n",
    "\n",
    "Supposing two input channels, 'train' and 'test', were used in the call to the `SKLearn` estimator's `fit()` method, the following environment variables will be set, following the format `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "* `SM_CHANNEL_TRAIN`: A string representing the path to the directory containing data in the 'train' channel\n",
    "* `SM_CHANNEL_TEST`: Same as above, but for the 'test' channel.\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance. For example, the script that we will run in this notebook is the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize 'scikit_learn_iris.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the Scikit-learn container imports your training script, you should always put your training code in a main guard `(if __name__=='__main__':)` so that the container does not inadvertently run your training code at the wrong point in execution.\n",
    "\n",
    "For more information about training environment variables, please visit https://github.com/aws/sagemaker-containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Scikit Estimator <a class=\"anchor\" id=\"create_sklearn_estimator\"></a>\n",
    "\n",
    "To run our Scikit-learn training script on SageMaker, we construct a `sagemaker.sklearn.estimator.sklearn` estimator, which accepts several constructor arguments:\n",
    "\n",
    "* __entry_point__: The path to the Python script SageMaker runs for training and prediction.\n",
    "* __role__: Role ARN\n",
    "* __train_instance_type__ *(optional)*: The type of SageMaker instances for training. __Note__: Because Scikit-learn does not natively support GPU training, Sagemaker Scikit-learn does not currently support training on GPU instance types.\n",
    "* __sagemaker_session__ *(optional)*: The session used to train on Sagemaker.\n",
    "* __hyperparameters__ *(optional)*: A dictionary passed to the train function as hyperparameters.\n",
    "\n",
    "To see the code for the SKLearn Estimator, see here: https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instance_type='ml.m5.xlarge'\n",
    "\n",
    "instance_type='local'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Reference: \n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/sklearn.html\n",
    "\n",
    "Supported Scikit-learn version\tMinimum Python version\n",
    "1.2-1\t3.8\n",
    "1.0-1\t3.7\n",
    "0.23-1\t3.6\n",
    "0.20.0\t2.7 or 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Reference: http://sagemaker.readthedocs.io/en/latest/estimators.html\n",
    "\n",
    "# SDK 2.x version does not require train prefix for instance count and type\n",
    "# Specify framework and python Version\n",
    "\n",
    "# Updated framework version from 0.20.0 to 0.23-1\n",
    "\n",
    "estimator = SKLearn(entry_point='scikit_learn_iris.py',\n",
    "                    framework_version = \"0.23-1\",\n",
    "                    py_version = 'py3',\n",
    "                    instance_type= instance_type,                     \n",
    "                    role=role, \n",
    "                    output_path=model_data_uri,\n",
    "                    base_job_name='sklearn-iris',\n",
    "                    hyperparameters={'n_estimators': 50,'max_depth':5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SKLearn Estimator on Iris data <a class=\"anchor\" id=\"train_sklearn\"></a>\n",
    "Training is very simple, just call `fit` on the Estimator! This will start a SageMaker Training job that will download the data for us, invoke our scikit-learn code (in the provided script file), and save any model artifacts that the script creates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit({'training':training_data_uri,'testing':testing_data_uri})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.latest_training_job.job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the trained model to make inference requests <a class=\"anchor\" id=\"inference\"></a>\n",
    "\n",
    "### Deploy the model <a class=\"anchor\" id=\"deploy\"></a>\n",
    "\n",
    "Deploying the model to SageMaker hosting just requires a `deploy` call on the fitted model. This call takes an instance count and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, \n",
    "                           instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose some data and use it for a prediction <a class=\"anchor\" id=\"prediction_request\"></a>\n",
    "\n",
    "In order to do some predictions, we'll extract some of the data we used for training and do predictions against it. This is, of course, bad statistical practice, but a good way to see how the mechanism works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(test_file, names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Class Labels to integers\n",
    "# Labeled Classes\n",
    "labels=[0,1,2]\n",
    "classes = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df.iloc[:,1:]\n",
    "print(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_class'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Confusion Matrix</h2>\n",
    "Confusion Matrix is a table that summarizes performance of classification model.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: \n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(df['encoded_class'],\n",
    "                              df['predicted_class'],labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes,\n",
    "                      title='Confusion matrix - Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes,\n",
    "                      title='Confusion matrix - Count',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(\n",
    "    df['encoded_class'],\n",
    "    df['predicted_class'],\n",
    "    labels=labels,\n",
    "    target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endpoint cleanup <a class=\"anchor\" id=\"endpoint_cleanup\"></a>\n",
    "\n",
    "When you're done with the endpoint, you'll want to clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDK 2\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another way to deploy endpoint\n",
    "## Using trained model artifacts\n",
    "https://sagemaker.readthedocs.io/en/stable/sagemaker.sklearn.html#scikit-learn-predictor\n",
    "https://sagemaker.readthedocs.io/en/stable/using_sklearn.html#working-with-existing-model-data-and-training-jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated framework version from 0.20.0 to 0.23-1\n",
    "import sagemaker.sklearn\n",
    "\n",
    "model = sagemaker.sklearn.model.SKLearnModel(model_data=model_data,\n",
    "                                             role=role, entry_point='scikit_learn_iris.py',\n",
    "                                             framework_version = \"0.23-1\",\n",
    "                                             py_version = 'py3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_2 = model.deploy(initial_instance_count=1, instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_2.predict(X_test[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_2.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
